<!DOCTYPE html>
<html lang="en">
	<head>
		<link rel="icon" type="image/png" sizes="32x32" href="../favicons/favicon.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../favicons/favicon.png">
		<meta name="theme-color" content="#ffffff">

		<script>
			window.dataLayer = window.dataLayer || [];
		  	function gtag(){dataLayer.push(arguments);}
		  	gtag('js', new Date());
		  	gtag('config', 'UA-110543961-1');
		</script>

        <meta name="viewport" content="width=device-width, initial-scale=1.0">

		<meta name="description" content="Anthony Do - Portfolio; Machine Learning, Data Analysis">
		<meta name="keywords" content="Machine Learning, Data Analysis">
		<meta name="author" content="Anthony Do">

		<title>Anthony Do</title>

		<link href="../css/bootstrap.min.css" rel="stylesheet">
		<link href="../css/styles.css" rel="stylesheet">

		<script src="https://code.jquery.com/jquery-latest.js" type="text/javascript"></script>
		<script src="../js/jquery.isotope.js" type="text/javascript"></script>
		<script src="../js/filter.js" type="text/javascript"></script>
		<script src="../js/bootstrap.bundle.min.js"></script>
        <script src="../js/template.js" type="text/javascript" defer></script>
		<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Anthony Do | Data scientist &amp; mathematician knowledgeable in statistical analysis and visualizations to utilize the power of data. &lt;div class=&quot;center&quot;&gt; Resume LinkedIn Profile &lt;/div&gt;</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Anthony Do" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Data scientist &amp; mathematician knowledgeable in statistical analysis and visualizations to utilize the power of data. &lt;div class=&quot;center&quot;&gt; Resume LinkedIn Profile &lt;/div&gt;" />
<meta property="og:description" content="Data scientist &amp; mathematician knowledgeable in statistical analysis and visualizations to utilize the power of data. &lt;div class=&quot;center&quot;&gt; Resume LinkedIn Profile &lt;/div&gt;" />
<link rel="canonical" href="http://localhost:4000/projects/mnist-digits-analysis.html" />
<meta property="og:url" content="http://localhost:4000/projects/mnist-digits-analysis.html" />
<meta property="og:site_name" content="Anthony Do" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Anthony Do" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Data scientist &amp; mathematician knowledgeable in statistical analysis and visualizations to utilize the power of data. &lt;div class=&quot;center&quot;&gt; Resume LinkedIn Profile &lt;/div&gt;","headline":"Anthony Do","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/profile2.jpg"}},"url":"http://localhost:4000/projects/mnist-digits-analysis.html"}</script>
<!-- End Jekyll SEO tag -->

	</head>

  	<body>

        <div class="topnav">
			<portfolio-nav></portfolio-nav>
		</div>

        <div class="container">

			<br>
			<br>
			<br>

            <h1 id="mnist-handwritten-digits"><b>MNIST Handwritten Digits</b></h1>

            <hr>

            <p>The MNIST handwritten digit dataset consists of images of handwritten digits, together with labels indicating which digit is in each image. We can see that images are just matrices with scalar values so we can apply the different classification algorithms to them.</p>
            <p>Because both the features and the labels are present in this dataset (and labels for large datasets are generally difficult/expensive to obtain), this dataset is frequently used as a benchmark to compare various classification methods. For example, <a href="http://yann.lecun.com/exdb/mnist/">this webpage</a> gives a comparison of a variety of different classification methods on MNIST.</p>
            <p>Our goal is to use scikit-learn machine learning to compare classification methods on the MNIST dataset. </p>
            <p>There are several versions of the MNIST dataset. We used the one that is built-into scikit-learn, described <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">here</a>. </p>
            
            <ul>
                <li>Classes: 10 (one for each digit)</li>
                <li>Samples total: 1797</li>
                <li>Samples per class: <em>approx</em> 180</li>
                <li>Dimensionality: 64 (8 pixels by 8 pixels)</li>
                <li>Features: integers 0-16 (grayscale value; 0 is white, 16 is black)</li>
            </ul>

            <p>Here are some examples of the images. Note that the digits have been size-normalized and centered in a fixed-size (8 x 8 pixels) image.</p>

            <img src="http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_classification_001.png" class="center">

            <p>Note that we will scale the data before running it through our algorithms. <a href="http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling">This website</a> explains why scaling is important.</p>
            
            <br>

            <h2 id="exploratory-data-analysis"><b>Exploratory Data Analysis</b></h2>

            <hr>

            <pre><code>
<span class="hljs-built_in">Number</span> <span class="hljs-keyword">of</span> Digits: <span class="hljs-number">10</span>
<span class="hljs-built_in">Number</span> <span class="hljs-keyword">of</span> Samples: <span class="hljs-number">1797</span>
<span class="hljs-built_in">Number</span> <span class="hljs-keyword">of</span> Features: <span class="hljs-number">64</span>
            </code></pre>
            
            <img src="https://user-images.githubusercontent.com/29410712/180262496-99c8c1a6-9e38-4d39-83fd-16ba2bbe3e2e.png" alt="ExampleNumbers" class="center">

            <h2 id="classification-tools"><b>Classification Tools</b></h2>

            <hr>

            <h5 id="classification-with-support-vector-machines-svm-"><b>Classification with Support Vector Machines (SVM)</b></h5>

            <p>We developed a support-vector machine classification model for the data. In machine learning, support vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. It uses the support-vector classifier which splits the data with a soft margin (the distance between the observations and the threshold) and some misclassifications.</p>
            
            <ol>
                <li>Using Train-Test-Split validation procedure, we split the data into a training and a test set. </li>
                <li>We use SVM to build a classifier using the <em>training dataset</em>.</li>
                <li>We will find the misclassified digits.</li>
                <li>Using the &#39;cross_val_score&#39; function, we evaluate the accuracy of the SVM for 100 different values of the parameter C between 1 and 500.</li>
                <li>We will train and test the algorithm on the raw (non-scaled) data.</li>
            </ol>

            <p>First, we split data into a training and a test set and then we use SVM with C = 100 to calculate the accuracy of the model.</p>

            <blockquote>
                <p><b>Python Code:</b></p>
            </blockquote>

            <pre><code>
X_train, X_test, y_train, <span class="hljs-attr">y_test</span> = train_test_split(X, y, <span class="hljs-attr">random_state=1,</span> <span class="hljs-attr">test_size=0.8)</span>
<span class="hljs-attr">clf</span> = svm.SVC(<span class="hljs-attr">kernel='rbf',</span> <span class="hljs-attr">C=100)</span>
clf.fit(X_train, y_train)

<span class="hljs-attr">y_pred</span> = clf.predict(X_test)
<span class="hljs-attr">accuracy</span> = metrics.accuracy_score(<span class="hljs-attr">y_true=y_test,</span> <span class="hljs-attr">y_pred=y_pred)</span>
<span class="hljs-attr">confusionmatrix</span> = metrics.confusion_matrix(<span class="hljs-attr">y_true=y_test,</span> <span class="hljs-attr">y_pred=y_pred)</span>
print(<span class="hljs-string">"Accuracy of Model:"</span>, accuracy)
            </code></pre>

            <blockquote>
                <p><b>Output:</b></p>
            </blockquote>
            
            <pre><code>
Accuracy <span class="hljs-keyword">of</span> Model: <span class="hljs-number">0.9534075104311543</span>
            </code></pre>
            
            <p>Since we are looking at the accuracy of the model predicting the digits, we will need to look at the misclassified digits. Using the code below, we find the index of each misclassified digit and then generate an image.</p>
            
            <blockquote>
                <p><b>Python Code:</b></p>
            </blockquote>

            <pre><code>
<span class="hljs-keyword">index</span> = <span class="hljs-number">0</span>
misclassified_images = []
<span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span>, predict <span class="hljs-keyword">in</span> zip(y_test, y_pred):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">label</span> != predict: 
        misclassified_images.append(<span class="hljs-keyword">index</span>)
    <span class="hljs-keyword">index</span> +=<span class="hljs-number">1</span>

plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))
plt.suptitle(<span class="hljs-string">'SVM Misclassified Digits'</span>);
<span class="hljs-keyword">for</span> plot_index, bad_index <span class="hljs-keyword">in</span> enumerate(misclassified_images[<span class="hljs-number">0</span>:]):
    p = plt.subplot(<span class="hljs-number">8</span>, <span class="hljs-number">9</span>, plot_index+<span class="hljs-number">1</span>)

    p.imshow(X_test[bad_index].reshape(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>), cmap=<span class="hljs-string">'Greys'</span>,
            interpolation=<span class="hljs-string">'nearest'</span>)
    p.set_xticks(()); p.set_yticks(())
            </code></pre>

            <br>
            
            <img src="https://user-images.githubusercontent.com/29410712/180263582-78a19b05-9fb4-4153-b639-9c56a732b522.png" alt="Misclassified-Digits-SVM" class="center">

            <p>Using Cross Validation to find the accuracy of SVM we get: </p>

            <pre><code>
                [<span class="hljs-number">0.93888889</span>, <span class="hljs-number">0.99444444</span>, <span class="hljs-number">0.98333333</span>, <span class="hljs-number">0.95</span>, <span class="hljs-number">0.98888889</span>, <span class="hljs-number">0.97777778</span>, <span class="hljs-number">0.97222222</span>, <span class="hljs-number">0.94972067</span>, <span class="hljs-number">0.96648045</span>, <span class="hljs-number">0.96089385</span>]
            </code></pre>
            
            <p>Thus, the highest accuracy using Cross Validation is 0.99.</p>
            <p>Now we look at the raw data and calculate the best C value for the SVM classification model.</p>

            <img src="https://user-images.githubusercontent.com/29410712/180265137-b223cfd3-dc44-47c5-9351-9ec87b5b5320.png" alt="SVM-Accuracy" class="center">

            <p>Using this code, we calculate the accuracy of the raw data.</p>

            <blockquote>
                <p><b>Python Code:</b></p>
            </blockquote>

            <pre><code>
<span class="hljs-attr">Xraw</span> = digits.data
<span class="hljs-attr">yraw</span> = digits.target
Xraw_train, Xraw_test, yraw_train, <span class="hljs-attr">yraw_test</span> = train_test_split(Xraw, yraw, <span class="hljs-attr">random_state=1,</span> <span class="hljs-attr">test_size=0.8)</span>
<span class="hljs-attr">clfraw</span> = svm.SVC(<span class="hljs-attr">kernel='rbf',</span> <span class="hljs-attr">C=10)</span>
clfraw.fit(Xraw_train, yraw_train)

<span class="hljs-attr">yraw_pred</span> = clfraw.predict(Xraw_test)
<span class="hljs-attr">accuracyraw</span> = metrics.accuracy_score(<span class="hljs-attr">y_true=yraw_test,</span> <span class="hljs-attr">y_pred=yraw_pred)</span>
print(<span class="hljs-string">"Accuracy of Raw Model:"</span>, accuracyraw)
            </code></pre>
            
            <blockquote>
                <p><b>Output:</b></p>
            </blockquote>

            <pre><code>
<span class="hljs-keyword">Accuracy</span> of <span class="hljs-keyword">Raw</span> Model: <span class="hljs-number">0.9756606397774688</span>
            </code></pre>

            <br>

            <p>Using the SVM classifier with C = 100, the accuracy score is 0.95 with the scaled training data. The most common mistake that the SVM classifier makes is in cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform. The SVM classifier does not perform very well when the data set has more noise such that the target classes are overlapping. The best score from the &#39;cross_val_score&#39; function is the score of 0.99. With the raw or unscaled data, the accuracy score is 0.975. Additionally, based on the chart, the best C value is 5.</p>
            
            <br>
            
            <h5 id="prediction-with-k-nearest-neighbors-k-nn-"><b>Prediction with K-Nearest Neighbors (K-NN)</b></h5>

            <p>The K-nearest neighbors (K-NN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. It categorizes unknown variables into different clusters. In our study, we will try out different values to choose the value of K. Low values of K can be noisy and subject to the effects of outliers. Large values of K smooth over things, but you don&#39;t want K to be so large that a category with only a few samples in it will always be outvoted by other categories. We will be using training data, data used for initial clustering (data where we know the categories in advance) to calculate the accuracy of the data.</p>
            <p>We developed a K-NN classification model for the data and use cross-validation to choose the best value of K. First, we will look at K = 10 and like before, we split the data into test and training sets.</p>
            
            <blockquote>
                <p><b>Python Code:</b></p>
            </blockquote>

            <pre><code>
            <span class="hljs-attr">k</span> = <span class="hljs-number">10</span>
            X_traink, X_testk, y_traink, <span class="hljs-attr">y_testk</span> = train_test_split(X, y, <span class="hljs-attr">random_state=1,</span> <span class="hljs-attr">test_size=0.8)</span>
            <span class="hljs-attr">clfk</span> = KNeighborsClassifier(<span class="hljs-attr">n_neighbors=k)</span>
            clfk.fit(X_traink, y_traink)
            
            <span class="hljs-attr">y_predk</span> = clfk.predict(X_testk)
            <span class="hljs-attr">accuracyk</span> = metrics.accuracy_score(<span class="hljs-attr">y_true=y_testk,</span> <span class="hljs-attr">y_pred=y_predk)</span>
            <span class="hljs-attr">confusionmatrix</span> = metrics.confusion_matrix(<span class="hljs-attr">y_true=y_testk,</span> <span class="hljs-attr">y_pred=y_predk)</span>
            print(<span class="hljs-string">"Accuracy of Model:"</span>, accuracyk)
            </code></pre>

            <blockquote>
                <p><b>Output:</b></p>
            </blockquote>
            
            <pre><code>
Accuracy <span class="hljs-keyword">of</span> Model: <span class="hljs-number">0.9123783031988874</span>
            </code></pre>
            
            <p>Since we are looking at the accuracy of the model predicting the digits, we will need to look at the misclassified digits. Using the code below, we find the index of each misclassified digit and then generate an image.</p>
            
            <blockquote>
                <p><b>Python Code:</b></p>
            </blockquote>

            <pre><code>
<span class="hljs-keyword">index</span> = <span class="hljs-number">0</span>
misclassified_imagesk = []
<span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span>, predict <span class="hljs-keyword">in</span> zip(y_testk, y_predk):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">label</span> != predict: 
        misclassified_imagesk.append(<span class="hljs-keyword">index</span>)
    <span class="hljs-keyword">index</span> +=<span class="hljs-number">1</span>

plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))
plt.suptitle(<span class="hljs-string">'K-NN Misclassified Digits'</span>);
<span class="hljs-keyword">for</span> plot_indexk, bad_indexk <span class="hljs-keyword">in</span> enumerate(misclassified_imagesk[<span class="hljs-number">0</span>:]):
    p = plt.subplot(<span class="hljs-number">9</span>, <span class="hljs-number">14</span>, plot_indexk+<span class="hljs-number">1</span>)

    p.imshow(X_test[bad_indexk].reshape(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>), cmap=<span class="hljs-string">'Greys'</span>,
            interpolation=<span class="hljs-string">'nearest'</span>)
    p.set_xticks(()); p.set_yticks(())
            </code></pre>

            <br>
            
            <img src="https://user-images.githubusercontent.com/29410712/180266882-8fdc8b31-31f0-4db3-b311-369bce66b113.png" alt="Misclassified-Digits-KNN" class="center">

            <p>Using Cross Validation to find the accuracy of K-NN we get: </p>

            <pre><code>
[<span class="hljs-number">0.91666667</span>, <span class="hljs-number">0.96111111</span>, <span class="hljs-number">0.96666667</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.95</span>, <span class="hljs-number">0.96666667</span>, <span class="hljs-number">0.97222222</span>, <span class="hljs-number">0.95530726</span>, <span class="hljs-number">0.95530726</span>, <span class="hljs-number">0.93854749</span>]
            </code></pre>
            
            <p>Thus, the highest accuracy using Cross Validation is 0.97.</p>
            <p>Now we look at the raw data and calculate the best K value for the K-NN classification model.</p>

            <img src="https://user-images.githubusercontent.com/29410712/180267282-5355c9b0-03a7-4664-bd68-4c4fa85c95e2.png" alt="K-NN-Accuracy" class="center">

            <p>Using this code we calculate the accuracy of the raw data.</p>

            <blockquote>
                <p><b>Python Code:</b></p>
            </blockquote>

            <pre><code>
<span class="hljs-attr">lfk</span> = KNeighborsClassifier(<span class="hljs-attr">n_neighbors=1)</span>
clfk.fit(X_traink, y_traink)

<span class="hljs-attr">y_predk</span> = clfk.predict(X_testk)
<span class="hljs-attr">accuracyk</span> = metrics.accuracy_score(<span class="hljs-attr">y_true=y_testk,</span> <span class="hljs-attr">y_pred=y_predk)</span>
print(<span class="hljs-string">"Accuracy of Raw Model:"</span>, accuracyk)
            </code></pre>

            <blockquote>
                <p><b>Output:</b></p>
            </blockquote>
            
            <pre><code>
<span class="hljs-keyword">Accuracy</span> of <span class="hljs-keyword">Raw</span> Model: <span class="hljs-number">0.9666203059805285</span>
            </code></pre>
            
            <br>

            <p>Using the K-NN, we would need to manually remove missing values and outliers. The K-NN classifier could potentially generate wrong predictions because of the lack of scaling. The best score from the &#39;cross_val_score&#39; function is the score of 0.97 of the test accuracy. With the raw or unscaled data, the best accuracy score is achieved with K = 1 and it is 0.967.</p>
            
            
        </div>
    </body>

    <footer-component></footer-component>

</html>